% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/predict.R
\name{predict.textmodel_lss}
\alias{predict.textmodel_lss}
\title{Prediction method for textmodel_lss}
\usage{
\method{predict}{textmodel_lss}(
  object,
  newdata = NULL,
  se.fit = FALSE,
  density = FALSE,
  rescaling = TRUE,
  min_n = 1L,
  ...
)
}
\arguments{
\item{object}{a fitted LSS textmodel}

\item{newdata}{a dfm on which prediction should be made}

\item{se.fit}{if \code{TRUE}, returns standard error of document scores.}

\item{density}{if \code{TRUE}, returns frequency of polarity words in documents.}

\item{rescaling}{if \code{TRUE}, normalizes polarity scores using \code{scale()}.}

\item{min_n}{set the minimum number of polarity words in documents.}

\item{...}{not used}
}
\description{
Prediction method for textmodel_lss
}
\details{
Polarity scores of documents are the means of polarity scores of
words weighted by their frequency. When \code{se.fit = TRUE}, this function
returns the weighted means, their standard errors, and the number of
polarity words in the documents. When \code{rescaling = TRUE}, it
converts the raw polarity scores to z sores for easier interpretation.

Documents tend to receive extreme polarity scores when they have only few
polarity word. This is problematic when LSS is applied to short documents
(e.g. social media posts) or individual sentences, but users can alleviate
this problem by setting the expected length of "normal" documents to
\code{min_n}. This stetting does not affect empty or longer documents.
}
