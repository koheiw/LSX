[{"path":"/articles/pkgdown/application.html","id":"preperation","dir":"Articles > Pkgdown","previous_headings":"","what":"Preperation","title":"Application in research","text":"analyze corpus introduction, pre-processing. use dictionary keywords example.","code":"corp <- readRDS(\"data_corpus_sputnik2022.rds\") |>     corpus_reshape() toks <- tokens(corp, remove_punct = TRUE, remove_symbols = TRUE,                 remove_numbers = TRUE, remove_url = TRUE) dfmt <- dfm(toks) |>      dfm_remove(stopwords(\"en\")) dict <- dictionary(file = \"dictionary.yml\") print(dict[c(\"country\", \"energy\")]) #> Dictionary object with 2 primary key entries and 2 nested levels. #> - [country]: #>   - [us]: #>     - united states, us, american*, washington #>   - [uk]: #>     - united kingdom, uk, british, london #>   - [eu]: #>     - european union, eu, european*, brussels #>   - [se]: #>     - sweden, swedish, stockholm #>   - [fi]: #>     - finland, finnish, helsinki #>   - [ua]: #>     - ukraine, ukrainian*, kiev, kyiv #>   [ reached max_nkey ... 1 more key ] #> - [energy]: #>   - gas, oil, engery"},{"path":"/articles/pkgdown/application.html","id":"estimate-the-polarity-of-words","dir":"Articles > Pkgdown","previous_headings":"","what":"Estimate the polarity of words","title":"Application in research","text":"measure sentiment specifically energy issues, collect words occur frequently around keywords “oil”, “gas”, “energy” passing terms. keywords called target words.","code":"seed <- as.seedwords(data_dictionary_sentiment) term <- char_context(toks, pattern = dict$energy, p = 0.01) lss <- textmodel_lss(dfmt, seeds = seed, terms = term, cache = TRUE,                       include_data = TRUE, group_data = TRUE) textplot_terms(lss)"},{"path":"/articles/pkgdown/application.html","id":"predict-the-polarity-of-documents","dir":"Articles > Pkgdown","previous_headings":"","what":"Predict the polarity of documents","title":"Application in research","text":"can extract document variables DFM LSS model save predicted polarity scores new variable.","code":"dat <- docvars(lss$data) dat$lss <- predict(lss) print(nrow(dat)) #> [1] 8063"},{"path":"/articles/pkgdown/application.html","id":"detect-the-mentions-of-countryregion","dir":"Articles > Pkgdown","previous_headings":"","what":"Detect the mentions of country/region","title":"Application in research","text":"can detect mentions countries using dictionary. want classify texts country accurately, use newsmap package. can create dummy variables mentions country/region dfm_group(dfmt_dict) > 0. must group documents unit analysis articles example (recall textmodel_lss(group_data = TRUE) ).","code":"dfmt_dict <- dfm(tokens_lookup(toks, dict$country[c(\"us\", \"eu\")])) print(head(dfmt_dict)) #> Document-feature matrix of: 6 documents, 2 features (91.67% sparse) and 4 docvars. #>                features #> docs            us eu #>   s1092644731.1  2  0 #>   s1092644731.2  0  0 #>   s1092644731.3  0  0 #>   s1092644731.4  0  0 #>   s1092644731.5  0  0 #>   s1092644731.6  0  0 mat <- as.matrix(dfm_group(dfmt_dict) > 0) print(head(mat)) #>              features #> docs             us    eu #>   s1092644731  TRUE FALSE #>   s1092643478  TRUE  TRUE #>   s1092643372 FALSE FALSE #>   s1092643164  TRUE FALSE #>   s1092641413  TRUE FALSE #>   s1092640142  TRUE FALSE dat <- cbind(dat, mat)"},{"path":"/articles/pkgdown/application.html","id":"results","dir":"Articles > Pkgdown","previous_headings":"","what":"Results","title":"Application in research","text":"must smooth polarity scores documents separately country/region using smooth_lss(). smoothing, can see difference US EU expanded soon proposition European Gas Demand Reduction Plan.  test changes proposition statistically significant, create dummy variable period proposition perform regression analysis interactions country/region dummies. akin difference--differences design often employ analysis news (Watanabe 2017; Watanabe et al. 2022). dat_war contains scores since beginning war, intercept average sentiment articles without mentions US EU proposition war; usTRUE euTRUE average sentiment articles mentions US EU period, respectively. coefficient afterTRUE indicates overall sentiment became negative proposition (β = -0.11; p < 0.01). insignificant coefficient euTRUE:afterTRUE shows sentiment EU also decreased, large positive coefficient usTRUE:afterTRUE suggests sentiment US increased (β = 0.22; p < 0.001) became positive proposition.","code":"smo_us <- smooth_lss(subset(dat, us), lss_var = \"lss\", date_var = \"date\") smo_us$country <- \"us\" smo_eu <- smooth_lss(subset(dat, eu), lss_var = \"lss\", date_var = \"date\") smo_eu$country <- \"eu\" smo <- rbind(smo_us, smo_eu) ggplot(smo, aes(x = date, y = fit, color = country)) +      geom_line() +     geom_ribbon(aes(ymin = fit - se.fit * 1.96, ymax = fit + se.fit * 1.96, fill = country),                  alpha = 0.1, colour = NA) +     geom_vline(xintercept = as.Date(\"2022-06-26\"), linetype = \"dotted\") +     scale_x_date(date_breaks = \"months\", date_labels = \"%b\") +     labs(title = \"Sentiment on energy\", x = \"Date\", y = \"Sentiment\",           fill = \"Country\", color = \"Country\") dat_war <- subset(dat, date >= as.Date(\"2022-02-24\")) dat_war$after <- dat_war$date >= as.Date(\"2022-06-20\") summary(dat_war[c(\"lss\", \"us\", \"eu\", \"after\")]) #>       lss               us              eu            after         #>  Min.   :-5.76915   Mode :logical   Mode :logical   Mode :logical   #>  1st Qu.:-0.67069   FALSE:2448      FALSE:4544      FALSE:3394      #>  Median :-0.02243   TRUE :4698      TRUE :2602      TRUE :3752      #>  Mean   :-0.03344                                                   #>  3rd Qu.: 0.56654                                                   #>  Max.   : 7.39866                                                   #>  NA's   :3 reg <- lm(lss ~ us + eu + after + us * after + eu * after, dat_war) summary(reg)  #>  #> Call: #> lm(formula = lss ~ us + eu + after + us * after + eu * after,  #>     data = dat_war) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -5.8331 -0.6280  0.0082  0.5974  7.3347  #>  #> Coefficients: #>                  Estimate Std. Error t value Pr(>|t|)     #> (Intercept)       0.06392    0.03234   1.976  0.04816 *   #> usTRUE           -0.11098    0.03740  -2.968  0.00301 **  #> euTRUE           -0.09021    0.03659  -2.465  0.01371 *   #> afterTRUE        -0.11935    0.04385  -2.722  0.00651 **  #> usTRUE:afterTRUE  0.22883    0.05062   4.521 6.27e-06 *** #> euTRUE:afterTRUE -0.02401    0.04983  -0.482  0.62991     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 1.006 on 7137 degrees of freedom #>   (3 observations deleted due to missingness) #> Multiple R-squared:  0.005494,   Adjusted R-squared:  0.004797  #> F-statistic: 7.885 on 5 and 7137 DF,  p-value: 2.043e-07"},{"path":"/articles/pkgdown/application.html","id":"conclusions","dir":"Articles > Pkgdown","previous_headings":"","what":"Conclusions","title":"Application in research","text":"analysis shows Sputnik covered energy issues US positively EU negatively proposition European Gas Demand Reduction Plan. findings preliminary, can give tentative interpretation: Russian government attempted create divisions US EU emphasizing different impact Ukraine war sanctions Russia American European lives.","code":""},{"path":"/articles/pkgdown/application.html","id":"references","dir":"Articles > Pkgdown","previous_headings":"","what":"References","title":"Application in research","text":"Watanabe, K. (2017). Measuring news bias: Russia’s official news agency ITAR-TASS’ coverage Ukraine crisis. European Journal Communication. https://doi.org/10.1177/0267323117695735. Watanabe, K., Segev, E., & Tago, . (2022). Discursive diversion: Manipulation nuclear threats conservative leaders Japan Israel, International Communication Gazette. https://doi.org/10.1177/17480485221097967.","code":""},{"path":"/articles/pkgdown/introduction.html","id":"preperation","dir":"Articles > Pkgdown","previous_headings":"","what":"Preperation","title":"Introduction to LSX","text":"can download corpus Sputnik articles Ukraine. must segment documents sentences using corpus_reshape() perform LSS. reshaping corpus allows LSS accurately estimate semantic similarity words. Otherwise, can follow standard procedure pre-processing removing punctuation marks grammatical words (“stop words”).","code":"library(LSX) library(quanteda) #> Package version: 3.2.5 #> Unicode version: 14.0 #> ICU version: 70.1 #> Parallel computing: 4 of 4 threads used. #> See https://quanteda.io for tutorials and examples. library(ggplot2) corp <- readRDS(\"data_corpus_sputnik2022.rds\") |>     corpus_reshape(to = \"sentences\") toks <- tokens(corp, remove_punct = TRUE, remove_symbols = TRUE,                 remove_numbers = TRUE, remove_url = TRUE) dfmt <- dfm(toks) |>      dfm_remove(stopwords(\"en\"))"},{"path":"/articles/pkgdown/introduction.html","id":"example-1-generic-sentiment","dir":"Articles > Pkgdown","previous_headings":"","what":"Example 1: generic sentiment","title":"Introduction to LSX","text":"basic usage LSS predicting sentiment documents. can estimate sentiment articles corpus running commands LSX package.","code":""},{"path":"/articles/pkgdown/introduction.html","id":"estimate-the-polarity-of-words","dir":"Articles > Pkgdown","previous_headings":"Example 1: generic sentiment","what":"Estimate the polarity of words","title":"Introduction to LSX","text":"data_dictionary_sentiment built-dictionary sentiment seed words. .seedwords() converts dictionary object named numeric vector, numbers indicate seed words’ polarity (positive negative). Taking DFM seed words inputs, textmodel_lss() computes polarity scores words corpus based semantic similarity seed words. usually need change value k (300 default). include_data group_data TRUE (FALSE default), internally applies dfm_group() x group sentences original documents, effectively reversing segmentation corpus_reshape(), save grouped DFM LSS object lss$data. encourage use caching mechanism speed execution function second time. cache = TRUE, intermediate object saved folder lss_cache working directory. can delete old cache files taking much space storage. can visualize polarity words using textplot_terms(). highlighted = NULL, randomly samples 50 words highlights .  can also manually specify words (emoji) highlight. can pass glob patterns highlighted want.","code":"seed <- as.seedwords(data_dictionary_sentiment) print(seed) #>        good        nice   excellent    positive   fortunate     correct  #>           1           1           1           1           1           1  #>    superior         bad       nasty        poor    negative unfortunate  #>           1          -1          -1          -1          -1          -1  #>       wrong    inferior  #>          -1          -1 lss <- textmodel_lss(dfmt, seeds = seed, k = 300, cache = TRUE,                       include_data = TRUE, group_data = TRUE) textplot_terms(lss, highlighted = NULL) emoji <- featnames(dfm_select(dfmt, \"^\\\\p{Emoji_Presentation}+$\", valuetype = \"regex\")) textplot_terms(lss, highlighted = c(emoji, names(seed)))"},{"path":"/articles/pkgdown/introduction.html","id":"predict-the-polarity-of-documents","dir":"Articles > Pkgdown","previous_headings":"Example 1: generic sentiment","what":"Predict the polarity of documents","title":"Introduction to LSX","text":"Yon can compute polarity scores documents using predict(). best workflow copying document variables DFM LSS object adding predicted polarity scores new variable data frame. visualize polarity documents, need smooth scores using smooth_lss(). plot shows sentiment articles Ukraine became negative March positive April. Zero Y-axis overall mean score; dotted vertical line indicate beginning war.","code":"dat <- docvars(lss$data) dat$lss <- predict(lss) print(nrow(dat)) #> [1] 8063 smo <- smooth_lss(dat, lss_var = \"lss\", date_var = \"date\") ggplot(smo, aes(x = date, y = fit)) +      geom_line() +     geom_ribbon(aes(ymin = fit - se.fit * 1.96, ymax = fit + se.fit * 1.96), alpha = 0.1) +     geom_vline(xintercept = as.Date(\"2022-02-24\"), linetype = \"dotted\") +     scale_x_date(date_breaks = \"months\", date_labels = \"%b\") +     labs(title = \"Sentiment about Ukraine\", x = \"Date\", y = \"Sentiment\")"},{"path":"/articles/pkgdown/introduction.html","id":"example-2-natos-hostility","dir":"Articles > Pkgdown","previous_headings":"","what":"Example 2: NATO’s hostility","title":"Introduction to LSX","text":"can measure specific concepts train LSS custom seed words. Adopting seed words earlier study geopolitical threat (Trubowitz & Watanabe,2021), estimate NATO’s hostility expressed Sputnik articles. can download dictionary file Github repository.","code":"dict <- dictionary(file = \"dictionary.yml\") print(dict$hostility) #> Dictionary object with 2 key entries. #> - [hostile]: #>   - adversary, adversaries, enemy, enemies, foe, foes, hostile #> - [friendly]: #>   - aid, aids, friends, friend, ally, allies, peaceful"},{"path":"/articles/pkgdown/introduction.html","id":"estimate-the-polarity-of-words-1","dir":"Articles > Pkgdown","previous_headings":"Example 2: NATO’s hostility","what":"Estimate the polarity of words","title":"Introduction to LSX","text":"target NATO’s hostility, assign polarity scores words occur around “nato” modifiers. can collect words using char_context() pass textmodel_lss() terms. group_data set FALSE want analyze sentence example. Note tokens_remove() used exclude county city names polarity words (otherwise “sweden” “finland” become strong polarity words). Since selected NATO-related terms LSS model, words plot fewer first example.","code":"seed2 <- as.seedwords(dict$hostility) term <- tokens_remove(toks, dict$country, padding = TRUE) |>      char_context(pattern = \"nato\", p = 0.01) lss2 <- textmodel_lss(dfmt, seeds = seed2, terms = term, cache = TRUE,                        include_data = TRUE, group_data = FALSE) textplot_terms(lss2)"},{"path":"/articles/pkgdown/introduction.html","id":"predict-the-polarity-of-documents-1","dir":"Articles > Pkgdown","previous_headings":"Example 2: NATO’s hostility","what":"Predict the polarity of documents","title":"Introduction to LSX","text":"compute polarity scores documents using predict() min_n avoid short sentences receive extremely large negative positive scores (.e. outliers). predict() tends return extreme scores short sentences computes polarity documents based polarity words weighted frequency. prevent small number words determining document scores, set min_n = 10, roughly first quantile sentence lengths. can use smooth_lss() visualize scores, engine “locfit” data frame 10 thousands scores. plot clearly shows NATO’s hostility expressed Sputnik articles surged response Sweden’s Finland’s decisions join alliance May 16 17, respectively.","code":"dat2 <- docvars(lss2$data) quantile(ntoken(lss2$data)) #>   0%  25%  50%  75% 100%  #>    0    9   14   20  261 dat2$lss <- predict(lss2, min_n = 10) print(nrow(dat2)) #> [1] 150459 smo2 <- smooth_lss(dat2, lss_var = \"lss\", date_var = \"date\", engine = \"locfit\") ggplot(smo2, aes(x = date, y = fit)) +      geom_line() +     geom_ribbon(aes(ymin = fit - se.fit * 1.96, ymax = fit + se.fit * 1.96), alpha = 0.1) +     geom_vline(xintercept = as.Date(\"2022-05-16\"), linetype = \"dotted\") +     scale_x_date(date_breaks = \"months\", date_labels = \"%b\") +     labs(title = \"NATO's hostility\", x = \"Date\", y = \"Hostility\")"},{"path":"/articles/pkgdown/introduction.html","id":"references","dir":"Articles > Pkgdown","previous_headings":"Example 2: NATO’s hostility","what":"References","title":"Introduction to LSX","text":"Trubowitz, P., & Watanabe, K. (2021). Geopolitical Threat Index: Text-Based Computational Approach Identifying Foreign Threats. International Studies Quarterly, https://doi.org/10.1093/isq/sqab029. Watanabe, K. (2021). Latent Semantic Scaling: Semisupervised Text Analysis Technique New Domains Languages, Communication Methods Measures, https://doi.org/10.1080/19312458.2020.1832976.","code":""},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Kohei Watanabe. Author, maintainer, copyright holder.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Watanabe K (2023). LSX: Semi-Supervised Algorithm Document Scaling. R package version 1.3.0.","code":"@Manual{,   title = {LSX: Semi-Supervised Algorithm for Document Scaling},   author = {Kohei Watanabe},   year = {2023},   note = {R package version 1.3.0}, }"},{"path":"/index.html","id":"lsx-semi-supervised-algorithm-for-document-scaling","dir":"","previous_headings":"","what":"Semi-Supervised Algorithm for Document Scaling","title":"Semi-Supervised Algorithm for Document Scaling","text":"quantitative text analysis, cost training supervised machine learning models tend high corpus large. LSS semi-supervised document scaling technique developed perform large scale analysis low cost. Taking user-provided seed words weak supervision, estimates polarity words corpus latent semantic analysis locates documents unidimensional scale (e.g. sentiment).","code":""},{"path":"/index.html","id":"install","dir":"","previous_headings":"","what":"Install","title":"Semi-Supervised Algorithm for Document Scaling","text":"CRAN: Github:","code":"install.packages(\"LSX\") devtools::install_github(\"koheiw/LSX\")"},{"path":"/index.html","id":"examples","dir":"","previous_headings":"","what":"Examples","title":"Semi-Supervised Algorithm for Document Scaling","text":"Please visit package website examples: Introduction LSX Application research Please read following papers algorithm methodology, application non-English texts (Japanese Hebrew): Watanabe, Kohei. 2020. “Latent Semantic Scaling: Semisupervised Text Analysis Technique New Domains Languages”, Communication Methods Measures. Watanabe, Kohei, Segev, Elad, & Tago, Atsushi. (2022). “Discursive diversion: Manipulation nuclear threats conservative leaders Japan Israel”, International Communication Gazette.","code":""},{"path":"/index.html","id":"other-publications","dir":"","previous_headings":"","what":"Other publications","title":"Semi-Supervised Algorithm for Document Scaling","text":"LSS used research various fields social science. Nakamura, Kentaro. 2022 Balancing Opportunities Incentives: Rising China’s Mediated Public Diplomacy Changes Crisis, International Journal Communication. Zollinger, Delia. 2022 Cleavage Identities Voters’ Words: Harnessing Open-Ended Survey Responses, American Journal Political Science. Brändle, Verena K., Olga Eisele. 2022. “Thin Line: Governmental Border Communication Times European Crises” Journal Common Market Studies. Umansky, Natalia. 2022. “gets say ? Speaking security social media”. New Media & Society. Rauh, Christian, 2022. “Supranational emergency politics? executives’ public crisis communication may tell us”, Journal European Public Policy. Trubowitz, Peter Watanabe, Kohei. 2021. “Geopolitical Threat Index: Text-Based Computational Approach Identifying Foreign Threats”, International Studies Quarterly. Vydra, Simon Kantorowicz, Jaroslaw. 2020. “Tracing Policy-relevant Information Social Media: Case Twitter COVID-19 Crisis”. Statistics, Politics Policy. Watanabe, Kohei. 2017. “Measuring News Bias: Russia’s Official News Agency ITAR-TASS’s Coverage Ukraine Crisis”, European Journal Communication. publications available Google Scholar.","code":""},{"path":"/reference/as.coefficients_textmodel.html","id":null,"dir":"Reference","previous_headings":"","what":"Coerce various objects to coefficients_textmodel\nThis is a helper function used in summary.textmodel_*. — as.coefficients_textmodel","title":"Coerce various objects to coefficients_textmodel\nThis is a helper function used in summary.textmodel_*. — as.coefficients_textmodel","text":"Coerce various objects coefficients_textmodel helper function used summary.textmodel_*.","code":""},{"path":"/reference/as.coefficients_textmodel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coerce various objects to coefficients_textmodel\nThis is a helper function used in summary.textmodel_*. — as.coefficients_textmodel","text":"","code":"as.coefficients_textmodel(x)"},{"path":"/reference/as.coefficients_textmodel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coerce various objects to coefficients_textmodel\nThis is a helper function used in summary.textmodel_*. — as.coefficients_textmodel","text":"x object coerced","code":""},{"path":"/reference/as.seedwords.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a list or a dictionary to seed words — as.seedwords","title":"Convert a list or a dictionary to seed words — as.seedwords","text":"Convert list dictionary seed words","code":""},{"path":"/reference/as.seedwords.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a list or a dictionary to seed words — as.seedwords","text":"","code":"as.seedwords(x, upper = 1, lower = 2, concatenator = \"_\")"},{"path":"/reference/as.seedwords.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a list or a dictionary to seed words — as.seedwords","text":"x list characters vectors dictionary object. upper numeric index key seed words higher scores. lower numeric index key seed words lower scores. concatenator character replace separators multi-word seed words.","code":""},{"path":"/reference/as.seedwords.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a list or a dictionary to seed words — as.seedwords","text":"named numeric vector seed words polarity scores","code":""},{"path":"/reference/as.statistics_textmodel.html","id":null,"dir":"Reference","previous_headings":"","what":"Coerce various objects to statistics_textmodel — as.statistics_textmodel","title":"Coerce various objects to statistics_textmodel — as.statistics_textmodel","text":"helper function used summary.textmodel_*.","code":""},{"path":"/reference/as.statistics_textmodel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coerce various objects to statistics_textmodel — as.statistics_textmodel","text":"","code":"as.statistics_textmodel(x)"},{"path":"/reference/as.statistics_textmodel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coerce various objects to statistics_textmodel — as.statistics_textmodel","text":"x object coerced","code":""},{"path":"/reference/as.summary.textmodel.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign the summary.textmodel class to a list — as.summary.textmodel","title":"Assign the summary.textmodel class to a list — as.summary.textmodel","text":"Assign summary.textmodel class list","code":""},{"path":"/reference/as.summary.textmodel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign the summary.textmodel class to a list — as.summary.textmodel","text":"","code":"as.summary.textmodel(x)"},{"path":"/reference/as.summary.textmodel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assign the summary.textmodel class to a list — as.summary.textmodel","text":"x named list","code":""},{"path":"/reference/as.textmodel_lss.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a dummy textmodel_lss object from external objects — as.textmodel_lss","title":"Create a dummy textmodel_lss object from external objects — as.textmodel_lss","text":"Create dummy textmodel_lss object numeric vector, dense matrix existing textmodel_lss object. Pre-trained word-embedding models used perform LSS function.","code":""},{"path":"/reference/as.textmodel_lss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a dummy textmodel_lss object from external objects — as.textmodel_lss","text":"","code":"as.textmodel_lss(x, ...)"},{"path":"/reference/as.textmodel_lss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a dummy textmodel_lss object from external objects — as.textmodel_lss","text":"x object dummy textmodel_lss object created. ... arguments used create dummy object. seeds must given x dense matrix.","code":""},{"path":"/reference/as.textmodel_lss.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a dummy textmodel_lss object from external objects — as.textmodel_lss","text":"dummy textmodel_lss object","code":""},{"path":"/reference/as.textmodel_lss.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a dummy textmodel_lss object from external objects — as.textmodel_lss","text":"named numeric vector dense matrix set beta embedding respectively. dense matrix column names words.","code":""},{"path":"/reference/as.textmodel_lss.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a dummy textmodel_lss object from external objects — as.textmodel_lss","text":"","code":"v <- c(\"a\" = 0.1, \"z\" = -0.2, \"d\" = 0.3, \"h\" = -0.05) lss <- as.textmodel_lss(v)"},{"path":"/reference/coef.textmodel_lss.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract model coefficients from a fitted textmodel_lss object — coef.textmodel_lss","title":"Extract model coefficients from a fitted textmodel_lss object — coef.textmodel_lss","text":"coef() extract model coefficients fitted textmodel_lss object.  coefficients() alias.","code":""},{"path":"/reference/coef.textmodel_lss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract model coefficients from a fitted textmodel_lss object — coef.textmodel_lss","text":"","code":"# S3 method for textmodel_lss coef(object, ...)  coefficients.textmodel_lss(object, ...)"},{"path":"/reference/coef.textmodel_lss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract model coefficients from a fitted textmodel_lss object — coef.textmodel_lss","text":"object fitted textmodel_lss object. ... used.","code":""},{"path":"/reference/cohesion.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes cohesion of components of latent semantic analysis — cohesion","title":"Computes cohesion of components of latent semantic analysis — cohesion","text":"Computes cohesion components latent semantic analysis","code":""},{"path":"/reference/cohesion.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes cohesion of components of latent semantic analysis — cohesion","text":"","code":"cohesion(x, bandwidth = 10)"},{"path":"/reference/cohesion.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes cohesion of components of latent semantic analysis — cohesion","text":"x fitted textmodel_lss bandwidth size window smoothing","code":""},{"path":"/reference/data_dictionary_ideology.html","id":null,"dir":"Reference","previous_headings":"","what":"Seed words for analysis of left-right political ideology — data_dictionary_ideology","title":"Seed words for analysis of left-right political ideology — data_dictionary_ideology","text":"Seed words analysis left-right political ideology","code":""},{"path":"/reference/data_dictionary_ideology.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Seed words for analysis of left-right political ideology — data_dictionary_ideology","text":"","code":"as.seedwords(data_dictionary_ideology) #>   deficit austerity  unstable recession inflation  currency workforce      poor  #>         1         1         1         1         1         1         1        -1  #>   poverty      free  benefits    prices     money   workers  #>        -1        -1        -1        -1        -1        -1"},{"path":"/reference/data_dictionary_sentiment.html","id":null,"dir":"Reference","previous_headings":"","what":"Seed words for analysis of positive-negative sentiment — data_dictionary_sentiment","title":"Seed words for analysis of positive-negative sentiment — data_dictionary_sentiment","text":"Seed words analysis positive-negative sentiment","code":""},{"path":"/reference/data_dictionary_sentiment.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Seed words for analysis of positive-negative sentiment — data_dictionary_sentiment","text":"Turney, P. D., & Littman, M. L. (2003). Measuring Praise Criticism: Inference Semantic Orientation Association. ACM Trans. Inf. Syst., 21(4), 315–346. doi:10.1145/944012.944013","code":""},{"path":"/reference/data_dictionary_sentiment.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Seed words for analysis of positive-negative sentiment — data_dictionary_sentiment","text":"","code":"as.seedwords(data_dictionary_sentiment) #>        good        nice   excellent    positive   fortunate     correct  #>           1           1           1           1           1           1  #>    superior         bad       nasty        poor    negative unfortunate  #>           1          -1          -1          -1          -1          -1  #>       wrong    inferior  #>          -1          -1"},{"path":"/reference/data_textmodel_lss_russianprotests.html","id":null,"dir":"Reference","previous_headings":"","what":"A fitted LSS model on street protest in Russia — data_textmodel_lss_russianprotests","title":"A fitted LSS model on street protest in Russia — data_textmodel_lss_russianprotests","text":"model trained Russian media corpus (newspapers, TV transcripts newswires) analyze framing street protests. scale protests \"freedom expression\" (high) vs \"social disorder\" (low). Although slots missing object (model imported original Python implementation), allows scale texts using predict.","code":""},{"path":"/reference/data_textmodel_lss_russianprotests.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"A fitted LSS model on street protest in Russia — data_textmodel_lss_russianprotests","text":"Lankina, Tomila, Kohei Watanabe. “'Russian Spring' 'Spring Betrayal'? Media Mirror Putin's Evolving Strategy Ukraine.” Europe-Asia Studies 69, . 10 (2017): 1526–56. doi:10.1080/09668136.2017.1397603 .","code":""},{"path":"/reference/diagnosys.html","id":null,"dir":"Reference","previous_headings":"","what":"Identify noisy documents in a corpus — diagnosys","title":"Identify noisy documents in a corpus — diagnosys","text":"Identify noisy documents corpus","code":""},{"path":"/reference/diagnosys.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Identify noisy documents in a corpus — diagnosys","text":"","code":"diagnosys(x, ...)"},{"path":"/reference/diagnosys.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Identify noisy documents in a corpus — diagnosys","text":"x character corpus object whose texts diagnosed. ... extra arguments passed tokens.","code":""},{"path":"/reference/predict.textmodel_lss.html","id":null,"dir":"Reference","previous_headings":"","what":"Prediction method for textmodel_lss — predict.textmodel_lss","title":"Prediction method for textmodel_lss — predict.textmodel_lss","text":"Prediction method textmodel_lss","code":""},{"path":"/reference/predict.textmodel_lss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prediction method for textmodel_lss — predict.textmodel_lss","text":"","code":"# S3 method for textmodel_lss predict(   object,   newdata = NULL,   se_fit = FALSE,   density = FALSE,   rescale = TRUE,   cut = NULL,   min_n = 0L,   ... )"},{"path":"/reference/predict.textmodel_lss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prediction method for textmodel_lss — predict.textmodel_lss","text":"object fitted LSS textmodel. newdata dfm prediction made. se_fit TRUE, returns standard error document scores. density TRUE, returns frequency polarity words documents. rescale TRUE, normalizes polarity scores using scale(). cut vector one two percentile values dichotomized polarty scores words. two values given, words receive zero polarity. min_n set minimum number polarity words documents. ... used","code":""},{"path":"/reference/predict.textmodel_lss.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prediction method for textmodel_lss — predict.textmodel_lss","text":"Polarity scores documents means polarity scores words weighted frequency. se_fit = TRUE, function returns weighted means, standard errors, number polarity words documents. rescale = TRUE, converts raw polarity scores z sores easier interpretation. rescale = FALSE cut used, polarity scores documents bounded [-1.0, 1.0]. Documents tend receive extreme polarity scores polarity words. problematic LSS applied short documents (e.g. social media posts) individual sentences, users can alleviate problem adding zero polarity words short documents using min_n. setting affect empty documents.","code":""},{"path":"/reference/print.coefficients_textmodel.html","id":null,"dir":"Reference","previous_headings":"","what":"Print methods for textmodel features estimates\nThis is a helper function used in print.summary.textmodel. — print.coefficients_textmodel","title":"Print methods for textmodel features estimates\nThis is a helper function used in print.summary.textmodel. — print.coefficients_textmodel","text":"Print methods textmodel features estimates helper function used print.summary.textmodel.","code":""},{"path":"/reference/print.coefficients_textmodel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print methods for textmodel features estimates\nThis is a helper function used in print.summary.textmodel. — print.coefficients_textmodel","text":"","code":"# S3 method for coefficients_textmodel print(x, digits = max(3L, getOption(\"digits\") - 3L), ...)"},{"path":"/reference/print.coefficients_textmodel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print methods for textmodel features estimates\nThis is a helper function used in print.summary.textmodel. — print.coefficients_textmodel","text":"x coefficients_textmodel object digits minimal number significant digits, see print.default() ... additional arguments used","code":""},{"path":"/reference/print.statistics_textmodel.html","id":null,"dir":"Reference","previous_headings":"","what":"Implements print methods for textmodel_statistics — print.statistics_textmodel","title":"Implements print methods for textmodel_statistics — print.statistics_textmodel","text":"Implements print methods textmodel_statistics","code":""},{"path":"/reference/print.statistics_textmodel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Implements print methods for textmodel_statistics — print.statistics_textmodel","text":"","code":"# S3 method for statistics_textmodel print(x, digits = max(3L, getOption(\"digits\") - 3L), ...)"},{"path":"/reference/print.statistics_textmodel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Implements print methods for textmodel_statistics — print.statistics_textmodel","text":"x textmodel_wordscore_statistics object digits minimal number significant digits, see print.default() ... arguments passed methods","code":""},{"path":"/reference/print.summary.textmodel.html","id":null,"dir":"Reference","previous_headings":"","what":"print method for summary.textmodel — print.summary.textmodel","title":"print method for summary.textmodel — print.summary.textmodel","text":"print method summary.textmodel","code":""},{"path":"/reference/print.summary.textmodel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"print method for summary.textmodel — print.summary.textmodel","text":"","code":"# S3 method for summary.textmodel print(x, digits = max(3L, getOption(\"digits\") - 3L), ...)"},{"path":"/reference/print.summary.textmodel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"print method for summary.textmodel — print.summary.textmodel","text":"x summary.textmodel object digits minimal number significant digits, see print.default() ... additional arguments used","code":""},{"path":"/reference/seedwords.html","id":null,"dir":"Reference","previous_headings":"","what":"Seed words for Latent Semantic Analysis — seedwords","title":"Seed words for Latent Semantic Analysis — seedwords","text":"Seed words Latent Semantic Analysis","code":""},{"path":"/reference/seedwords.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Seed words for Latent Semantic Analysis — seedwords","text":"","code":"seedwords(type)"},{"path":"/reference/seedwords.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Seed words for Latent Semantic Analysis — seedwords","text":"type type seed words currently sentiment (sentiment) political ideology (ideology).","code":""},{"path":"/reference/seedwords.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Seed words for Latent Semantic Analysis — seedwords","text":"Turney, P. D., & Littman, M. L. (2003). Measuring Praise Criticism: Inference Semantic Orientation Association. ACM Trans. Inf. Syst., 21(4), 315–346. doi:10.1145/944012.944013","code":""},{"path":"/reference/seedwords.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Seed words for Latent Semantic Analysis — seedwords","text":"","code":"seedwords('sentiment') #>        good        nice   excellent    positive   fortunate     correct  #>           1           1           1           1           1           1  #>    superior         bad       nasty        poor    negative unfortunate  #>           1          -1          -1          -1          -1          -1  #>       wrong    inferior  #>          -1          -1"},{"path":"/reference/smooth_lss.html","id":null,"dir":"Reference","previous_headings":"","what":"Smooth predicted LSS scores by local polynomial regression — smooth_lss","title":"Smooth predicted LSS scores by local polynomial regression — smooth_lss","text":"Smooth predicted LSS scores local polynomial regression","code":""},{"path":"/reference/smooth_lss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Smooth predicted LSS scores by local polynomial regression — smooth_lss","text":"","code":"smooth_lss(   x,   lss_var = \"fit\",   date_var = \"date\",   span = 0.1,   from = NULL,   to = NULL,   engine = c(\"loess\", \"locfit\"),   ... )"},{"path":"/reference/smooth_lss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Smooth predicted LSS scores by local polynomial regression — smooth_lss","text":"x data.frame containing LSS scores dates. lss_var name column LSS scores. date_var name columns dates. span determines level smoothing. start time period. end time period. engine specifies function smooth LSS scores: loess() locfit(). latter used n > 10000. ... extra arguments passed loess() lp()","code":""},{"path":"/reference/textmodel_lss.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a Latent Semantic Scaling model — textmodel_lss","title":"Fit a Latent Semantic Scaling model — textmodel_lss","text":"Latent Semantic Scaling (LSS) word embedding-based semisupervised algorithm document scaling.","code":""},{"path":"/reference/textmodel_lss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a Latent Semantic Scaling model — textmodel_lss","text":"","code":"textmodel_lss(x, ...)  # S3 method for dfm textmodel_lss(   x,   seeds,   terms = NULL,   k = 300,   slice = NULL,   weight = \"count\",   cache = FALSE,   simil_method = \"cosine\",   engine = c(\"RSpectra\", \"irlba\", \"rsvd\"),   auto_weight = FALSE,   include_data = FALSE,   group_data = FALSE,   verbose = FALSE,   ... )  # S3 method for fcm textmodel_lss(   x,   seeds,   terms = NULL,   w = 50,   max_count = 10,   weight = \"count\",   cache = FALSE,   simil_method = \"cosine\",   engine = c(\"rsparse\"),   auto_weight = FALSE,   verbose = FALSE,   ... )"},{"path":"/reference/textmodel_lss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a Latent Semantic Scaling model — textmodel_lss","text":"x dfm fcm created quanteda::dfm() quanteda::fcm() ... additional arguments passed underlying engine. seeds character vector named numeric vector contains seed words. seed words contain \"*\", interpreted glob patterns. See quanteda::valuetype. terms character vector named numeric vector specify words polarity scores computed; numeric vector, words' polarity scores weighted accordingly; NULL, features quanteda::dfm() quanteda::fcm() used. k number singular values requested SVD engine. used x dfm. slice number indices components word vectors used compute similarity; slice < k truncate word vectors; useful diagnosys simulation. weight weighting scheme passed quanteda::dfm_weight(). Ignored engine \"rsparse\". cache TRUE, save result SVD next execution identical x settings. Use base::options(lss_cache_dir) change location cache files save. simil_method specifies method compute similarity features. value passed quanteda.textstats::textstat_simil(), \"cosine\" used otherwise. engine select engine factorize x generate word vectors. Choose RSpectra::svds(), irlba::irlba(), rsvd::rsvd(), rsparse::GloVe(). auto_weight automatically determine weights approximate polarity terms seed words. See details. include_data TRUE, fitted model includes dfm supplied x. group_data TRUE, apply dfm_group(x) saving dfm. verbose show messages TRUE. w size word vectors. Used x fcm. max_count passed x_max rsparse::GloVe$new() cooccurrence counts ceiled threshold. changed according size corpus. Used x fcm.","code":""},{"path":"/reference/textmodel_lss.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit a Latent Semantic Scaling model — textmodel_lss","text":"Latent Semantic Scaling (LSS) semisupervised document scaling method. textmodel_lss() constructs word vectors use-provided documents (x) weights words (terms) based semantic proximity seed words (seeds). Seed words known polarity words (e.g. sentiment words) users manually choose. required number seed words usually 5 10 end scale. seeds named numeric vector positive negative values, bipolar LSS model construct; seeds character vector, unipolar LSS model. Usually bipolar models perform better document scaling ends scale defined user. seed word's polarity score computed textmodel_lss() tends diverge original score given user score affected original score also original scores seed words. auto_weight = TRUE, original scores weighted automatically using stats::optim() minimize squared difference seed words' computed original scores. Weighted scores saved seed_weighted object. Please visit package website examples.","code":""},{"path":"/reference/textmodel_lss.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fit a Latent Semantic Scaling model — textmodel_lss","text":"Watanabe, Kohei. 2020. \"Latent Semantic Scaling: Semisupervised Text Analysis Technique New Domains Languages\", Communication Methods Measures. doi:10.1080/19312458.2020.1832976 . Watanabe, Kohei. 2017. \"Measuring News Bias: Russia's Official News Agency ITAR-TASS' Coverage Ukraine Crisis\" European Journal Communication. doi:10.1177/0267323117695735 .","code":""},{"path":"/reference/textplot_components.html","id":null,"dir":"Reference","previous_headings":"","what":"[experimental] Plot clusters of word vectors — textplot_components","title":"[experimental] Plot clusters of word vectors — textplot_components","text":"Experimental function find clusters word vectors","code":""},{"path":"/reference/textplot_components.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"[experimental] Plot clusters of word vectors — textplot_components","text":"","code":"textplot_components(   x,   n = 5,   method = \"ward.D2\",   scale = c(\"absolute\", \"relative\") )"},{"path":"/reference/textplot_components.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"[experimental] Plot clusters of word vectors — textplot_components","text":"x fitted textmodel_lss. n number cluster. method method hierarchical clustering. scale change scale y-axis.","code":""},{"path":"/reference/textplot_simil.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot similarity between seed words — textplot_simil","title":"Plot similarity between seed words — textplot_simil","text":"Plot similarity seed words","code":""},{"path":"/reference/textplot_simil.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot similarity between seed words — textplot_simil","text":"","code":"textplot_simil(x)"},{"path":"/reference/textplot_simil.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot similarity between seed words — textplot_simil","text":"x fitted textmodel_lss object.","code":""},{"path":"/reference/textplot_terms.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot polarity scores of words — textplot_terms","title":"Plot polarity scores of words — textplot_terms","text":"Plot polarity scores words","code":""},{"path":"/reference/textplot_terms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot polarity scores of words — textplot_terms","text":"","code":"textplot_terms(x, highlighted = NULL, max_highlighted = 50, max_words = 10000)"},{"path":"/reference/textplot_terms.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot polarity scores of words — textplot_terms","text":"x fitted textmodel_lss object. highlighted quanteda::pattern select words highlight. max_highlighted maximum number words highlight. highlighted = NULL, words highlight randomly selected proportionally polarity ^ 2 * log(frequency). max_words maximum number words plot. Words randomly sampled keep number limit.","code":""},{"path":"/reference/textstat_context.html","id":null,"dir":"Reference","previous_headings":"","what":"Identify context words using user-provided patterns — textstat_context","title":"Identify context words using user-provided patterns — textstat_context","text":"Identify context words using user-provided patterns","code":""},{"path":"/reference/textstat_context.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Identify context words using user-provided patterns — textstat_context","text":"","code":"textstat_context(   x,   pattern,   valuetype = c(\"glob\", \"regex\", \"fixed\"),   case_insensitive = TRUE,   window = 10,   min_count = 10,   remove_pattern = TRUE,   n = 1,   skip = 0,   ... )  char_context(   x,   pattern,   valuetype = c(\"glob\", \"regex\", \"fixed\"),   case_insensitive = TRUE,   window = 10,   min_count = 10,   remove_pattern = TRUE,   p = 0.001,   n = 1,   skip = 0 )"},{"path":"/reference/textstat_context.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Identify context words using user-provided patterns — textstat_context","text":"x tokens object created quanteda::tokens(). pattern quanteda::pattern() specify target words. valuetype type pattern matching: \"glob\" \"glob\"-style wildcard expressions; \"regex\" regular expressions; \"fixed\" exact matching. See quanteda::valuetype() details. case_insensitive TRUE, ignore case matching. window size window collocation analysis. min_count minimum frequency words within window considered collocations. remove_pattern TRUE, keywords contain target words. n integer vector specifying number elements concatenated n-gram.  element vector define \\(n\\) \\(n\\)-gram(s) produced. skip integer vector specifying adjacency skip size tokens forming n-grams, default 0 immediately neighbouring words. skipgrams, skip can vector integers, \"classic\" approach forming skip-grams set skip = \\(k\\) \\(k\\) distance \\(k\\) fewer skips used construct \\(n\\)-gram.  Thus \"4-skip-n-gram\" defined skip = 0:4 produces results include 4 skips, 3 skips, 2 skips, 1 skip, 0 skips (0 skips typical n-grams formed adjacent words).  See Guthrie et al (2006). ... additional arguments passed textstat_keyness(). p threshold statistical significance collocations.","code":""},{"path":[]},{"path":"/reference/weight_seeds.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal function to generate equally-weighted seed set — weight_seeds","title":"Internal function to generate equally-weighted seed set — weight_seeds","text":"Internal function generate equally-weighted seed set","code":""},{"path":"/reference/weight_seeds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal function to generate equally-weighted seed set — weight_seeds","text":"","code":"weight_seeds(seeds, type)"},{"path":"/news/index.html","id":"changes-in-v130","dir":"Changelog","previous_headings":"","what":"Changes in v1.3.0","title":"Changes in v1.3.0","text":"Move examples new package website: http://koheiw.github.io/LSX Rename “rescaling” “rescale” simplicity consistency. Improve random sampling words highlight textplot_terms() avoid congestion.","code":""},{"path":"/news/index.html","id":"changes-in-v120","dir":"Changelog","previous_headings":"","what":"Changes in v1.2.0","title":"Changes in v1.2.0","text":"CRAN release: 2022-12-04 Add group_data textmodel_lss() simplify workflow. Add max_highlighted textplot_terms() automatically highlight polarity words.","code":""},{"path":"/news/index.html","id":"changes-in-v114","dir":"Changelog","previous_headings":"","what":"Changes in v1.1.4","title":"Changes in v1.1.4","text":"Update .textmodel_lss() avoid errors textplot_terms() terms used.","code":""},{"path":"/news/index.html","id":"changes-in-v113","dir":"Changelog","previous_headings":"","what":"Changes in v1.1.3","title":"Changes in v1.1.3","text":"CRAN release: 2022-10-19 Restore examples textmodel_lss(). Defunct char_keyness() deprecated long.","code":""},{"path":"/news/index.html","id":"changes-in-v112","dir":"Changelog","previous_headings":"","what":"Changes in v1.1.2","title":"Changes in v1.1.2","text":"CRAN release: 2022-10-02 Update examples pass CRAN tests.","code":""},{"path":"/news/index.html","id":"changes-in-v111","dir":"Changelog","previous_headings":"","what":"Changes in v1.1.1","title":"Changes in v1.1.1","text":"CRAN release: 2022-02-26 Add min_n predict() make polarity scores short documents stable.","code":""},{"path":"/news/index.html","id":"changes-in-v110","dir":"Changelog","previous_headings":"","what":"Changes in v1.1.0","title":"Changes in v1.1.0","text":"CRAN release: 2022-02-24 Add .textmodel_lss() textmodel_lss objects allow modifying existing models. Allow terms textmodel_lss() named numeric vector give arbitrary weights.","code":""},{"path":"/news/index.html","id":"changes-in-v102","dir":"Changelog","previous_headings":"","what":"Changes in v1.0.2","title":"Changes in v1.0.2","text":"CRAN release: 2021-09-18 Add auto_weight argument textmodel_lss() .textmodel_lss() improve accuracy scaling. Remove group argument textplot_simil() simplify object. Make .seedwords() accept multiple indices upper lower.","code":""},{"path":"/news/index.html","id":"changes-in-v100","dir":"Changelog","previous_headings":"","what":"Changes in v1.0.0","title":"Changes in v1.0.0","text":"CRAN release: 2021-07-20 Add max_count textmodel_lss.fcm() passed x_max rsparse::GloVe$new(). Add max_words textplot_terms() avoid overcrowding. Make textplot_terms() work objects textmodel_lss.fcm(). Add concatenator .seedwords().","code":""},{"path":"/news/index.html","id":"changes-in-v099","dir":"Changelog","previous_headings":"","what":"Changes in v0.9.9","title":"Changes in v0.9.9","text":"CRAN release: 2021-04-19 Correct textstat_context() char_context() computes statistics. Deprecate char_keyness().","code":""},{"path":"/news/index.html","id":"changes-in-v098","dir":"Changelog","previous_headings":"","what":"Changes in v0.9.8","title":"Changes in v0.9.8","text":"CRAN release: 2021-03-22 Stop using functions arguments deprecated quanteda v3.0.0.","code":""},{"path":"/news/index.html","id":"changes-in-v097","dir":"Changelog","previous_headings":"","what":"Changes in v0.9.7","title":"Changes in v0.9.7","text":"CRAN release: 2021-03-08 Make .textmodel_lss.matrix() reliable. Remove quanteda.textplots dependencies.","code":""},{"path":"/news/index.html","id":"changes-in-v096","dir":"Changelog","previous_headings":"","what":"Changes in v0.9.6","title":"Changes in v0.9.6","text":"CRAN release: 2020-12-17 Updated reflect changes quanteda (creation quanteda.textstats).","code":""},{"path":"/news/index.html","id":"changes-in-v094","dir":"Changelog","previous_headings":"","what":"Changes in v0.9.4","title":"Changes in v0.9.4","text":"CRAN release: 2020-11-02 Fix char_context() always return frequent words context. Experimental textplot_factor() removed. .textmodel_lss() takes pre-trained word-embedding.","code":""},{"path":"/news/index.html","id":"changes-in-v093","dir":"Changelog","previous_headings":"","what":"Changes in v0.9.3","title":"Changes in v0.9.3","text":"Add textstat_context() char_context() replace char_keyness(). Make absolute sum seed weight equal 1.0 upper lower ends. textplot_terms() takes glob patterns character vector dictionary object. char_keyness() longer raise error patter found tokens object. Add engine smooth_lss() apply locfit() large datasets.","code":""},{"path":"/news/index.html","id":"changes-in-v092","dir":"Changelog","previous_headings":"","what":"Changes in v0.9.2","title":"Changes in v0.9.2","text":"CRAN release: 2020-09-22 Updated unit tests new versions stringi quanteda.","code":""},{"path":"/news/index.html","id":"changes-in-v090","dir":"Changelog","previous_headings":"","what":"Changes in v0.9.0","title":"Changes in v0.9.0","text":"CRAN release: 2020-09-09 Renamed LSS LSX CRAN submission.","code":""},{"path":"/news/index.html","id":"changes-in-v087","dir":"Changelog","previous_headings":"","what":"Changes in v0.8.7","title":"Changes in v0.8.7","text":"Added textplot_terms() improve visualization model terms.","code":""}]
